{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start you will need to download and unzip the competition data from Kaggle and ensure your directory structure looks like this\n",
    "```\n",
    "utils/\n",
    "    vgg16.py\n",
    "    utils.py\n",
    "lesson1/\n",
    "    redux.ipynb\n",
    "    data/\n",
    "        redux/\n",
    "            train/\n",
    "                cat.437.jpg\n",
    "                dog.9924.jpg\n",
    "                cat.1029.jpg\n",
    "                dog.4374.jpg\n",
    "            test/\n",
    "                231.jpg\n",
    "                325.jpg\n",
    "                1235.jpg\n",
    "                9923.jpg\n",
    "```\n",
    "\n",
    "You can download the data files from the competition page [here](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data) or you can download them from the command line using the [Kaggle CLI](https://github.com/floydwch/kaggle-cli).\n",
    "\n",
    "You should launch your notebook inside the lesson1 directory\n",
    "```\n",
    "cd lesson1\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/ubuntu/courses/deeplearning1/nbs/lesson1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cur_dir = os.getcwd()\n",
    "lesson_home_dir = cur_dir\n",
    "data_dir = cur_dir + '/data/redux'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#Allow relative imports to directories above lesson1/\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "#import modules\n",
    "from utils import *\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "#Instantiate plotting tool\n",
    "#In Jupyter notebooks, you will need to run this command before doing any plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Plan\n",
    "1. Create Validation and Sample sets - done\n",
    "2. Rearrange image files into their respective directories - done\n",
    "3. Finetune and Train model\n",
    "4. Generate predictions - done (wrote a function to gen pred file)\n",
    "5. Validate predictions - done (can get a prediction on a validation set)\n",
    "6. Submit predictions to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux\n"
     ]
    }
   ],
   "source": [
    "#Start by setting up the directories. This only should be run once\n",
    "\n",
    "%cd $data_dir\n",
    "%mkdir valid\n",
    "%mkdir results\n",
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/test\n",
    "%mkdir -p sample/valid\n",
    "%mkdir -p sample/results\n",
    "%mkdir -p test/unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/train\n"
     ]
    }
   ],
   "source": [
    "#Move 2000 training instances to the validation folder - only run once\n",
    "\n",
    "%cd $data_dir/train\n",
    "g = glob('*jpg') #gets a list of every file matching the pattern\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(2000):    \n",
    "    os.rename(shuf[i], data_dir + '/valid/' + shuf[i]) #renaming similar to mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#copy over 200,50 training and validation images to the sample folder - only run once\n",
    "from shutil import copyfile\n",
    "g = glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(250):\n",
    "    if i < 200:\n",
    "        copyfile(shuf[i], data_dir + '/sample/train/' + shuf[i])\n",
    "    else:\n",
    "        copyfile(shuf[i], data_dir +'/sample/valid/' + shuf[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/train\n",
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/valid\n",
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/sample/train\n",
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/sample/valid\n"
     ]
    }
   ],
   "source": [
    "#Now create a directory for cats and for dogs - only run once\n",
    "%cd $data_dir/train\n",
    "#%mkdir dogs\n",
    "#%mkdir cats\n",
    "%mv dog.*.jpg dogs/\n",
    "%mv cat.*.jpg cats/\n",
    "\n",
    "%cd $data_dir/valid\n",
    "#%mkdir dogs\n",
    "#%mkdir cats\n",
    "%mv dog.*.jpg dogs/\n",
    "%mv cat.*.jpg cats/\n",
    "\n",
    "%cd $data_dir/sample/train\n",
    "#%mkdir dogs\n",
    "#%mkdir cats\n",
    "%mv dog.*.jpg dogs/\n",
    "%mv cat.*.jpg cats/\n",
    "\n",
    "%cd $data_dir/sample/valid\n",
    "#%mkdir dogs\n",
    "#%mkdir cats\n",
    "%mv dog.*.jpg dogs/\n",
    "%mv cat.*.jpg cats/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/test\n"
     ]
    }
   ],
   "source": [
    "%cd $data_dir/test\n",
    "%mv *.jpg unknown/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set up packages and path\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/courses/deeplearning1/nbs/utils')\n",
    "from __future__ import division,print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#pre-trained model package\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "def getPath(sample):\n",
    "    #Takes in a boolean\n",
    "    if sample:\n",
    "        return \"/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/sample\"\n",
    "    else:\n",
    "        return \"/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def getPredsVal(mod, pred_path, batch_size = 8):\n",
    "    #val batches is a generator from gen.flow_from_directory. \n",
    "    val_batches = mod.get_batches(pred_path, shuffle=False, batch_size = batch_size, class_mode='binary')\n",
    "    #Preds is an array with [p_0, p_1]\n",
    "    preds = mod.model.predict_generator(val_batches, val_batches.nb_sample)  \n",
    "    #Need to better understand how much gets returned with the generator's next function\n",
    "    truth = val_batches.next()[1] \n",
    "    print(roc_auc_score(truth, preds[:, 1]))\n",
    "    print(log_loss(truth, preds[:, 1]))\n",
    "    return truth, preds[:, 1]\n",
    "\n",
    "def genSubmissions(mod, test_path, outfile):\n",
    "    #To get predictions\n",
    "    #Dogs are 2nd probability\n",
    "    test_batches, test_preds = vgg.test(test_path)\n",
    "    ids = [f.split('/')[1].split('.')[0] for f in test_batches.filenames]\n",
    "    df = pd.DataFrame({'id': ids, 'label': test_preds[:,1]}).sort(columns='id')\n",
    "    df.to_csv(res_path + '/'+ outfile, index=False)\n",
    "\n",
    "    \n",
    "batch_size = 64\n",
    "path = getPath(False)\n",
    "res_path = path + '/results'\n",
    "test_path = path + '/test'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "running epoch 0\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 586s - loss: 0.3778 - acc: 0.9678 - val_loss: 0.1582 - val_acc: 0.9835\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch 1\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 592s - loss: 0.3539 - acc: 0.9740 - val_loss: 0.2167 - val_acc: 0.9810\n",
      "Found 12500 images belonging to 1 classes.\n",
      "running epoch 2\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 604s - loss: 0.3427 - acc: 0.9759 - val_loss: 0.1956 - val_acc: 0.9865\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#Now train and fine tune, using code from lesson 1\n",
    "\n",
    "batch_size = 64\n",
    "path = getPath(False)\n",
    "res_path = path + '/results'\n",
    "test_path = path + '/test'\n",
    "\n",
    "\n",
    "vgg = Vgg16()\n",
    "# Grab a few images at a time for training and validation.\n",
    "# NB: They must be in subdirectories named based on their category\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "\n",
    "vgg.finetune(batches)\n",
    "\n",
    "#run multiple epochs\n",
    "nb_epochs = 3\n",
    "\n",
    "vgg.model.optimizer.lr = 0.01\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    print('running epoch {}'.format(i))\n",
    "    vgg.fit(batches, val_batches, nb_epoch = 1)\n",
    "    vgg.model.save_weights(res_path + 'dog_cats_{}_lrp01.h5'.format(i))\n",
    "    genSubmissions(vgg, test_path, 'dog_cats_{}_lrp01.csv'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 637s - loss: 0.1248 - acc: 0.9673 - val_loss: 0.0498 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "#Now train and fine tune top layer, but add regularization\n",
    "#Things to do: 1st add regularization to the last layer\n",
    "#Retrain last two dense layers\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "path = getPath(False)\n",
    "res_path = path + '/results'\n",
    "test_path = path + '/test'\n",
    "\n",
    "\n",
    "#First fine tune the model and fit it\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "vgg.finetune(batches)\n",
    "vgg.fit(batches, val_batches, nb_epoch = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.layers.core.Lambda'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'>\n",
      "<class 'keras.layers.convolutional.Convolution2D'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.core.Flatten'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Dropout'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Dropout'>\n",
      "<class 'keras.layers.core.Dense'>\n"
     ]
    }
   ],
   "source": [
    "#Now reset all of dense layer to trainable and refit\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "layers = vgg.model.layers\n",
    "for l in layers:\n",
    "    print(type(l))\n",
    "    \n",
    "dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "\n",
    "for layer in layers[dense_idx:]: \n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "vgg.fit(batches, val_batches, nb_epoch = True)\n",
    "vgg.model.save_weights(res_path + 'dog_cats_Retrain3.h5')\n",
    "genSubmissions(vgg, test_path, 'dog_cats_Retrain3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reset just the penultimate dense layer\n",
    "\n",
    "def trainDenseLayers(l):\n",
    "    '''\n",
    "    1 for last layer\n",
    "    2 for penultimate layer\n",
    "    3 for just before that\n",
    "    '''\n",
    "    vmod = Vgg16()\n",
    "    batches = vmod.get_batches(path + '/train', batch_size = batch_size)\n",
    "    val_batches = vmod.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "    vmod.finetune(batches)\n",
    "    vmod.fit(batches, val_batches, nb_epoch = 1)\n",
    "    layers = vgg.model.layers\n",
    "    if l > len(layers):\n",
    "        l = 0\n",
    "    dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][-l]\n",
    "    for layer in layers[dense_idx:]: \n",
    "        layer.trainable = True\n",
    "    vmod.fit(batches, val_batches, nb_epoch = 1)\n",
    "    return vmod\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 637s - loss: 0.1248 - acc: 0.9683 - val_loss: 0.0451 - val_acc: 0.9830\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 638s - loss: 0.0959 - acc: 0.9777 - val_loss: 0.0573 - val_acc: 0.9795\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "vgg2 = trainDenseLayers(2)\n",
    "vgg2.model.save_weights(res_path + 'dog_cats_Retrain2.h5')\n",
    "genSubmissions(vgg2, test_path, 'dog_cats_Retrain2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 637s - loss: 0.1177 - acc: 0.9696 - val_loss: 0.0502 - val_acc: 0.9850\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Don't fine tune last layer first\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "vgg.finetune(batches)\n",
    "layers = vgg.model.layers\n",
    "dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "for layer in layers[dense_idx:]: \n",
    "    layer.trainable = True\n",
    "\n",
    "vgg.fit(batches, val_batches, nb_epoch = 1)\n",
    "vgg.model.save_weights(res_path + 'dog_cats_Retrain_NoPre.h5')\n",
    "genSubmissions(vgg, test_path, 'dog_cats_Retrain_NoPre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Things to try\n",
    "- training convolutional layers\n",
    "- regularization instead of dropout (or in addition)\n",
    "- perturbing the images\n",
    "'''\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 635s - loss: 0.1493 - acc: 0.9679 - val_loss: 0.0493 - val_acc: 0.9860\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Instead of calling fine tune, add regularization to the last layer\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "vgg.finetune_reg(batches)\n",
    "vgg.fit(batches, val_batches, nb_epoch = 1)\n",
    "vgg.model.save_weights(res_path + 'dog_cats_regp01.h5')\n",
    "genSubmissions(vgg, test_path, 'dog_cats_regp01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 637s - loss: 0.1206 - acc: 0.9676 - val_loss: 0.0407 - val_acc: 0.9855\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Instead of calling fine tune, add regularization to the last layer\n",
    "#import vgg16; reload(vgg16)\n",
    "#from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "vgg.finetune_reg(batches, c = 0.001)\n",
    "vgg.fit(batches, val_batches, nb_epoch = 1)\n",
    "vgg.model.save_weights(res_path + 'dog_cats_regp001.h5')\n",
    "genSubmissions(vgg, test_path, 'dog_cats_regp001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 638s - loss: 0.2659 - acc: 0.9677 - val_loss: 0.0496 - val_acc: 0.9810\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Instead of calling fine tune, add regularization to the last layer\n",
    "#import vgg16; reload(vgg16)\n",
    "#from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "vgg.finetune_reg(batches, c = 0.1)\n",
    "vgg.fit(batches, val_batches, nb_epoch = 1)\n",
    "vgg.model.save_weights(res_path + 'dog_cats_regp1.h5')\n",
    "genSubmissions(vgg, test_path, 'dog_cats_regp1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 635s - loss: 0.2618 - acc: 0.9680 - val_loss: 0.0383 - val_acc: 0.9845\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Don't fine tune last layer first, retrain penultimate, and fit last with regularization\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "vgg.finetune_reg(batches, c = 0.1)\n",
    "layers = vgg.model.layers\n",
    "dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "for layer in layers[dense_idx:]: \n",
    "    layer.trainable = True\n",
    "\n",
    "vgg.fit(batches, val_batches, nb_epoch = 1)\n",
    "vgg.model.save_weights(res_path + 'dog_cats_Retrain_NoPre_regp1.h5')\n",
    "genSubmissions(vgg, test_path, 'dog_cats_Retrain_NoPre_regp1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 626s - loss: 0.4175 - acc: 0.9674 - val_loss: 0.0551 - val_acc: 0.9825\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:23: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Don't fine tune last layer first, retrain penultimate, and fit last with regularization\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "vgg.finetune_reg(batches, c = 1)\n",
    "layers = vgg.model.layers\n",
    "dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "for layer in layers[dense_idx:]: \n",
    "    layer.trainable = True\n",
    "\n",
    "vgg.fit(batches, val_batches, nb_epoch = 1)\n",
    "vgg.model.save_weights(res_path + 'dog_cats_Retrain_NoPre_reg1.h5')\n",
    "genSubmissions(vgg, test_path, 'dog_cats_Retrain_NoPre_reg1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.layers.core.Lambda'> False 0\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 1\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 2\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 3\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 4\n",
      "<class 'keras.layers.pooling.MaxPooling2D'> False 5\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 6\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 7\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 8\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 9\n",
      "<class 'keras.layers.pooling.MaxPooling2D'> False 10\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 11\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 12\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 13\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 14\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 15\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 16\n",
      "<class 'keras.layers.pooling.MaxPooling2D'> False 17\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 18\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 19\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 20\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 21\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 22\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 23\n",
      "<class 'keras.layers.pooling.MaxPooling2D'> False 24\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 25\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 26\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 27\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 28\n",
      "<class 'keras.layers.convolutional.ZeroPadding2D'> False 29\n",
      "<class 'keras.layers.convolutional.Convolution2D'> False 30\n",
      "<class 'keras.layers.pooling.MaxPooling2D'> False 31\n",
      "<class 'keras.layers.core.Flatten'> False 32\n",
      "<class 'keras.layers.core.Dense'> True 33\n",
      "<class 'keras.layers.core.Dropout'> True 34\n",
      "<class 'keras.layers.core.Dense'> True 35\n",
      "<class 'keras.layers.core.Dropout'> True 36\n",
      "<class 'keras.layers.core.Dense'> True 37\n"
     ]
    }
   ],
   "source": [
    "for i, l in enumerate(vgg.model.layers):\n",
    "    print('{} {} {}'.format(type(l),l.trainable, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "('Error allocating 589824 bytes of device memory (out of memory).', \"you might consider using 'theano.shared(..., borrow=True)'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-9564821232b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Don't fine tune last layer first, retrain last conv block as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/courses/deeplearning1/nbs/utils/vgg16.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFILE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://www.platform.ai/models/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/courses/deeplearning1/nbs/utils/vgg16.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/courses/deeplearning1/nbs/utils/vgg16.pyc\u001b[0m in \u001b[0;36mConvBlock\u001b[0;34m(self, layers, filters)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    306\u001b[0m                  output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    307\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Exception('All layers in a Sequential model '\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    485\u001b[0m                                     '`layer.build(batch_input_shape)`')\n\u001b[1;32m    486\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/convolutional.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid dim_ordering: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_ordering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{}_W'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{}_b'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/initializations.pyc\u001b[0m in \u001b[0;36mglorot_uniform\u001b[0;34m(shape, name, dim_ordering)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfan_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_ordering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim_ordering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfan_in\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/initializations.pyc\u001b[0m in \u001b[0;36muniform\u001b[0;34m(shape, scale, name)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36mrandom_uniform_variable\u001b[0;34m(shape, low, high, dtype, name)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrandom_uniform_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_FLOATX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     return variable(np.random.uniform(low=low, high=high, size=shape),\n\u001b[0;32m--> 141\u001b[0;31m                     dtype=dtype, name=name)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/sharedvalue.pyc\u001b[0m in \u001b[0;36mshared\u001b[0;34m(value, name, strict, allow_downcast, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 var = ctor(value, name=name, strict=strict,\n\u001b[0;32m--> 247\u001b[0;31m                            allow_downcast=allow_downcast, **kwargs)\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_tag_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/var.pyc\u001b[0m in \u001b[0;36mfloat32_shared_constructor\u001b[0;34m(value, name, strict, allow_downcast, borrow, broadcastable, target)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m# type.broadcastable is guaranteed to be a tuple, which this next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# function requires\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mdeviceval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_support_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcastable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: ('Error allocating 589824 bytes of device memory (out of memory).', \"you might consider using 'theano.shared(..., borrow=True)'\")"
     ]
    }
   ],
   "source": [
    "#Don't fine tune last layer first, retrain last conv block as well\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "vgg.finetune_reg(batches, c = 0.1)\n",
    "layers = vgg.model.layers\n",
    "dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "for layer in layers[25:]: \n",
    "    layer.trainable = True\n",
    "\n",
    "vgg.fit(batches, val_batches, nb_epoch = 1)\n",
    "vgg.model.save_weights(res_path + 'dog_cats_RetrainConv1_NoPre_regp1.h5')\n",
    "genSubmissions(vgg, test_path, 'dog_cats_RetrainConv1_NoPre_regp1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "<keras.layers.core.Lambda object at 0x7f5e6a9b5f10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e3646a9d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e643c5390>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e62937bd0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e62937790>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f5e3646aa50>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e6a9ac250>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e363b6990>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e6a9bf110>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e6acfa790>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f5e6a9acb50>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e68796950>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e721d2150>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e364d8d90>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e3649b2d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e35c795d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e35ca5390>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f5e6a9bf6d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e6a0d42d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e6bc56510>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e69d27b50>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e6b467c50>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e830b65d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e830b6b90>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f5e6a0d4a10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e7d33f2d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e68b52ed0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e35896fd0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e6770efd0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f5e627ec610>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f5e6b804750>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f5e7d33f6d0>\n",
      "<keras.layers.core.Flatten object at 0x7f5e69599090>\n",
      "<keras.layers.core.Dense object at 0x7f5e64b22050>\n",
      "<keras.layers.core.Dropout object at 0x7f5e64b22c10>\n",
      "<keras.layers.core.Dense object at 0x7f5e69599f90>\n",
      "<keras.layers.core.Dropout object at 0x7f5e6ba0b590>\n",
      "<keras.layers.core.Dense object at 0x7f5e63ec7050>\n",
      "Epoch 1/1\n",
      " 1344/23000 [>.............................] - ETA: 581s - loss: 3.6722 - acc: 0.7768"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-4041f6512442>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/courses/deeplearning1/nbs/utils/vgg16.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, batches, val_batches, nb_epoch)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         self.model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=nb_epoch,\n\u001b[0;32m--> 146\u001b[0;31m                 validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1442\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Add regularization to all of the dense layers\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "\n",
    "c = 0.1\n",
    "\n",
    "vgg.model.pop()\n",
    "\n",
    "for layer in vgg.model.layers: \n",
    "    layer.trainable=False\n",
    "    \n",
    "layers = vgg.model.layers\n",
    "dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "for layer in layers[dense_idx:]: \n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "vgg.model.add(Dense(batches.nb_class, activation='softmax', W_regularizer = l2(c)))\n",
    "\n",
    "vgg.compile()\n",
    "for layer in vgg.model.layers: \n",
    "    print(layer)\n",
    "\n",
    "vgg.fit(batches, val_batches, nb_epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'pop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-8cb0acaa3b0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'pop'"
     ]
    }
   ],
   "source": [
    "vgg.model.save_weights(res_path + 'dog_cats_RetrainConv1_NoPre_regp1.h5')\n",
    "genSubmissions(vgg, test_path, 'dog_cats_RetrainConv1_NoPre_regp1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
