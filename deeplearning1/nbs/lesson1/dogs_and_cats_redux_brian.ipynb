{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start you will need to download and unzip the competition data from Kaggle and ensure your directory structure looks like this\n",
    "```\n",
    "utils/\n",
    "    vgg16.py\n",
    "    utils.py\n",
    "lesson1/\n",
    "    redux.ipynb\n",
    "    data/\n",
    "        redux/\n",
    "            train/\n",
    "                cat.437.jpg\n",
    "                dog.9924.jpg\n",
    "                cat.1029.jpg\n",
    "                dog.4374.jpg\n",
    "            test/\n",
    "                231.jpg\n",
    "                325.jpg\n",
    "                1235.jpg\n",
    "                9923.jpg\n",
    "```\n",
    "\n",
    "You can download the data files from the competition page [here](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data) or you can download them from the command line using the [Kaggle CLI](https://github.com/floydwch/kaggle-cli).\n",
    "\n",
    "You should launch your notebook inside the lesson1 directory\n",
    "```\n",
    "cd lesson1\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/ubuntu/courses/deeplearning1/nbs/lesson1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cur_dir = os.getcwd()\n",
    "lesson_home_dir = cur_dir\n",
    "data_dir = cur_dir + '/data/redux'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#Allow relative imports to directories above lesson1/\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "#import modules\n",
    "from utils import *\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "#Instantiate plotting tool\n",
    "#In Jupyter notebooks, you will need to run this command before doing any plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Plan\n",
    "1. Create Validation and Sample sets - done\n",
    "2. Rearrange image files into their respective directories - done\n",
    "3. Finetune and Train model\n",
    "4. Generate predictions - done (wrote a function to gen pred file)\n",
    "5. Validate predictions - done (can get a prediction on a validation set)\n",
    "6. Submit predictions to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux\n"
     ]
    }
   ],
   "source": [
    "#Start by setting up the directories. This only should be run once\n",
    "\n",
    "%cd $data_dir\n",
    "%mkdir valid\n",
    "%mkdir results\n",
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/test\n",
    "%mkdir -p sample/valid\n",
    "%mkdir -p sample/results\n",
    "%mkdir -p test/unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/train\n"
     ]
    }
   ],
   "source": [
    "#Move 2000 training instances to the validation folder - only run once\n",
    "\n",
    "%cd $data_dir/train\n",
    "g = glob('*jpg') #gets a list of every file matching the pattern\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(2000):    \n",
    "    os.rename(shuf[i], data_dir + '/valid/' + shuf[i]) #renaming similar to mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#copy over 200,50 training and validation images to the sample folder - only run once\n",
    "from shutil import copyfile\n",
    "g = glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(250):\n",
    "    if i < 200:\n",
    "        copyfile(shuf[i], data_dir + '/sample/train/' + shuf[i])\n",
    "    else:\n",
    "        copyfile(shuf[i], data_dir +'/sample/valid/' + shuf[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/train\n",
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/valid\n",
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/sample/train\n",
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/sample/valid\n"
     ]
    }
   ],
   "source": [
    "#Now create a directory for cats and for dogs - only run once\n",
    "%cd $data_dir/train\n",
    "#%mkdir dogs\n",
    "#%mkdir cats\n",
    "%mv dog.*.jpg dogs/\n",
    "%mv cat.*.jpg cats/\n",
    "\n",
    "%cd $data_dir/valid\n",
    "#%mkdir dogs\n",
    "#%mkdir cats\n",
    "%mv dog.*.jpg dogs/\n",
    "%mv cat.*.jpg cats/\n",
    "\n",
    "%cd $data_dir/sample/train\n",
    "#%mkdir dogs\n",
    "#%mkdir cats\n",
    "%mv dog.*.jpg dogs/\n",
    "%mv cat.*.jpg cats/\n",
    "\n",
    "%cd $data_dir/sample/valid\n",
    "#%mkdir dogs\n",
    "#%mkdir cats\n",
    "%mv dog.*.jpg dogs/\n",
    "%mv cat.*.jpg cats/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/test\n"
     ]
    }
   ],
   "source": [
    "%cd $data_dir/test\n",
    "%mv *.jpg unknown/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "#set up packages and path\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/courses/deeplearning1/nbs/utils')\n",
    "from __future__ import division,print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#pre-trained model package\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "def getPath(sample):\n",
    "    #Takes in a boolean\n",
    "    if sample:\n",
    "        return \"/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux/sample\"\n",
    "    else:\n",
    "        return \"/home/ubuntu/courses/deeplearning1/nbs/lesson1/data/redux\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 591s - loss: 0.1224 - acc: 0.9666 - val_loss: 0.0446 - val_acc: 0.9850\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def getPredsVal(mod, pred_path, batch_size = 8):\n",
    "    #val batches is a generator from gen.flow_from_directory. \n",
    "    val_batches = mod.get_batches(pred_path, shuffle=False, batch_size = batch_size, class_mode='binary')\n",
    "    #Preds is an array with [p_0, p_1]\n",
    "    preds = mod.model.predict_generator(val_batches, val_batches.nb_sample)  \n",
    "    #Need to better understand how much gets returned with the generator's next function\n",
    "    truth = val_batches.next()[1] \n",
    "    print(roc_auc_score(truth, preds[:, 1]))\n",
    "    print(log_loss(truth, preds[:, 1]))\n",
    "    return truth, preds[:, 1]\n",
    "\n",
    "def genSubmissions(mod, test_path, outfile):\n",
    "    #To get predictions\n",
    "    #Dogs are 2nd probability\n",
    "    test_batches, test_preds = vgg.test(test_path)\n",
    "    ids = [f.split('/')[1].split('.')[0] for f in test_batches.filenames]\n",
    "    df = pd.DataFrame({'id': ids, 'label': test_preds[:,1]}).sort(columns='id')\n",
    "    df.to_csv(res_path + '/'+ outfile, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "running epoch 0\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 586s - loss: 0.3778 - acc: 0.9678 - val_loss: 0.1582 - val_acc: 0.9835\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch 1\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 592s - loss: 0.3539 - acc: 0.9740 - val_loss: 0.2167 - val_acc: 0.9810\n",
      "Found 12500 images belonging to 1 classes.\n",
      "running epoch 2\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 604s - loss: 0.3427 - acc: 0.9759 - val_loss: 0.1956 - val_acc: 0.9865\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#Now train and fine tune, using code from lesson 1\n",
    "\n",
    "batch_size = 64\n",
    "path = getPath(False)\n",
    "res_path = path + '/results'\n",
    "test_path = path + '/test'\n",
    "\n",
    "\n",
    "vgg = Vgg16()\n",
    "# Grab a few images at a time for training and validation.\n",
    "# NB: They must be in subdirectories named based on their category\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "\n",
    "vgg.finetune(batches)\n",
    "\n",
    "#run multiple epochs\n",
    "nb_epochs = 3\n",
    "\n",
    "vgg.model.optimizer.lr = 0.01\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    print('running epoch {}'.format(i))\n",
    "    vgg.fit(batches, val_batches, nb_epoch = 1)\n",
    "    vgg.model.save_weights(res_path + 'dog_cats_{}_lrp01.h5'.format(i))\n",
    "    genSubmissions(vgg, test_path, 'dog_cats_{}_lrp01.csv'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Now train and fine tune top layer, but add regularization\n",
    "#Things to do: 1st add regularization to the last layer\n",
    "#Retrain last two dense layers\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "path = getPath(False)\n",
    "res_path = path + '/results'\n",
    "test_path = path + '/test'\n",
    "\n",
    "\n",
    "vgg = Vgg16()\n",
    "# Grab a few images at a time for training and validation.\n",
    "# NB: They must be in subdirectories named based on their category\n",
    "batches = vgg.get_batches(path + '/train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + '/valid', batch_size = batch_size * 2)\n",
    "\n",
    "vgg.finetune(batches)\n",
    "\n",
    "#run multiple epochs\n",
    "nb_epochs = 1\n",
    "\n",
    "#vgg.model.optimizer.lr = 0.01\n",
    "\n",
    "#for i in range(nb_epochs):\n",
    "#    print('running epoch {}'.format(i))\n",
    "#    vgg.fit(batches, val_batches, nb_epoch = 1)\n",
    "#    vgg.model.save_weights(res_path + 'dog_cats_{}_lrp01.h5'.format(i))\n",
    "#    genSubmissions(vgg, test_path, 'dog_cats_{}_lrp01.csv'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2, activity_l2\n",
    "vgg.model.layers[-1].W_regularizer = l2(0.01)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "vgg.model.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
